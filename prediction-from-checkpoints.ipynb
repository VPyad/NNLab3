{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i1240_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1240_l512.ckpt\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"Но когда дело дошло до заказов, долговязый мужчина попросил стакан минералки\"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100        # Размер пакета\n",
    "num_steps = 100         # Шагов в пакете\n",
    "lstm_size = 512         # Количество LSTM юнитов в скрытом слое\n",
    "num_layers = 2          # Количество LSTM слоев\n",
    "learning_rate = 0.001   # Скорость обучения\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/stephen_king_it.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # Мы будем использовать эту же сеть для сэмплирования (генерации текста),\n",
    "        # при этом будем подавать по одному символу за один раз\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Получаем input placeholder'ы\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Строим LSTM ячейку\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Прогоняем данные через RNN слои\n",
    "        # Делаем one-hot кодирование входящих данных\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Прогоняем данные через RNN и собираем результаты\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Получаем предсказания (softmax) и результат logit-функции\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Считаем потери и оптимизируем (с обрезкой градиента)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Определяем placeholder'ы для входных, целевых данных, а также вероятности drop out\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        batch_size: Batch size, количество последовательностей в пакете\n",
    "        num_steps: Sequence length, сколько \"шагов\" делаем в пакете\n",
    "        \n",
    "    '''\n",
    "    # Объявляем placeholder'ы\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Placeholder для вероятности drop out\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Строим softmax слой и возвращаем результат его работы.\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        \n",
    "        x: Входящий от LSTM тензор\n",
    "        in_size: Размер входящего тензора, (кол-во LSTM юнитов скрытого слоя)\n",
    "        out_size: Размер softmax слоя (объем словаря)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # вытягиваем и решэйпим тензор, выполняя преобразование 3D -> 2D\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Соединяем результат LTSM слоев с softmax слоем\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Считаем logit-функцию\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    # Используем функцию softmax для получения предсказания\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Считаем функцию потери на основании значений logit-функции и целевых значений.\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        logits: значение logit-функции\n",
    "        targets: целевые значения, с которыми сравниваем предсказания\n",
    "        lstm_size: Количество юнитов в LSTM слое\n",
    "        num_classes: Количество классов в целевых значениях (размер словаря)\n",
    "        \n",
    "    '''\n",
    "    # Делаем one-hot кодирование целевых значений и решейпим по образу и подобию logits\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Считаем значение функции потери softmax cross entropy loss и возвращаем среднее значение\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Строим оптимизатор для обучения, используя обрезку градиента.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: значение функции потери\n",
    "        learning_rate: параметр скорости обучения\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Оптимизатор для обучения, обрезка градиента для контроля \"взрывающихся\" градиентов\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i200_l512.ckpt\n",
      "Но когда дело дошло до заказов, долговязый мужчина попросил стакан минералки пелелали на ветон селани стовосна кале нена провиниси санос сто вита нел сти като наме то нистенонась нос пело врилили пракал сто стов се палоло, ка налил сос та правана, и порадети, но проди подаль о стум слостони, к оридола, перутана валонь водолол вора слати стето нен весеть ва те прасталос стовани ка подерена, но дета, кестовали нала ном намо нали не педальлис полотьло пета намила.\n",
      "\n",
      "\n",
      " Нель поди стули насни не стот рали в ратот санотали, что с палелань ве теро продо сала сел насте перина волито ну вес подалил и кедаластола ва пиринала сто но са пасет ролана ста тато пел сено порулоста. Писто прано на прелали снелом вастини преснать илена. Ол недить сасли ва стуто тенос, она прулона, чеконати ватато стол ни но не носа тель, накони поделасиси ну вас вать пила вас на неле ногат пета наль, потити, сототь ото сновале, чостом ветон сот неми сесли не ститол сеноне, и толе сни пода нон ренета полиси поле таль о сто не велас порис слосно калас вонани перетоти стони стола ну прусань, от но ва\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab))\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i600_l512.ckpt\n",
      "Но когда дело дошло до заказов, долговязый мужчина попросил стакан минералки, но они сталала ворит не начил по не могу, наких казалось с поровом, накакал он в полому не в кресто в потене столони, что она себяла не постеринали сторен на нем поралать в текого поспедноги. Но положения не в куго перучитосто. В кототой с не порнали не вельчеть собя поделеть стурать, что в когда постратия в подум в домей, который семи от не порнила. Они том но водена староть восель стерилась следала пристать но в премому и подерели выдали в трума. В семом подамули, к ну восту ина ни все стревения в так не теля с него он совирол себя ни отручал на ведере вотом, на така в подвитал на в домую чаль оне сольцо на сколок служной кат стонь, который призивил они не в пакоми, которые вскака и верная воскатил не провить в как славетерь вередатальния подеть не столание сомотоние слушие.\n",
      "\n",
      "– То мене содер всемну непорнел он сливой вы стукала, и в тат потакая слеватились старом, подерома на семалась из постолово вольшик и протеми, которыя подомал в тетерой в дуска и нет собравали следилось. О подн\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i600_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab))\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i1200_l512.ckpt\n",
      "Но когда дело дошло до заказов, долговязый мужчина попросил стакан минералки.\n",
      "\n",
      "Откид игрулся под канимором, продрежился привелини с положащно с седоваде станому. Но в тем друг он удился в кане вышет с канаме из прадолка. Потым возворот под водом прошле, не выразнылась со дрегой словном всорнать.\n",
      "\n",
      "– То смогра тель на стала, подалаясь, потому что не все постоила его в колоденя.\n",
      "\n",
      "– Поставил он и пологив и сказала тебя на него но стала не следую, как обез вольстая.\n",
      "\n",
      "Потре передали его, как вернул сторону потрогом придавленных кровной, не покалился подумания, на тем, что они оставлесь в сколеко по пальной вольдой и поспили на том прослоше. Постоят он вернул со сторону она, когда он смогрил серебу соврись и неточку с пододавителество перекоритке, и не меня но отваловало стороны в траности и откратившесь с навости с ним, как водол стола с ободной времени, и продились на стаканут и плека на колеста, и вскотик в коросе, прошопелись сосказавить на тровенней-прока. Стэнл приклакила не строхо искраннуя подана, возмажая высклонно из кроватый постакина, в посторнене, потому \n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i1200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab))\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i1240_l512.ckpt\n",
      "Но когда дело дошло до заказов, долговязый мужчина попросил стакан минералки, но смотрел в темным. Стрики, ингалера в каталь и парно из мосте, которые не мог с сломить стеле..\n",
      "\n",
      "– Я дажа вы могу и подумать?\n",
      "\n",
      "– Давие то, что его могу не видить выследить начастника с парки. Прошен в столоне вызвинит, к нем, когда наборется отчастно в семней прошин.\n",
      "\n",
      "– Нет.\n",
      "\n",
      "– Ду. – Прошил во после тему, что, что он усили. Или началось в петеле от этом. Вы полома по вередение и од полосит собраться, и потом чем в этом не малоечный поробланки слова петел собретнее ни следовитель.\n",
      "\n",
      "Он совсем водром сторон в глаза.\n",
      "\n",
      "– Нет, не забывий. Я должен подель в том, что он в полей, почем на потом стоят, послу он посто в тель с нах. И ты направится в тебя и в этот, которая страха ни которых ночь с нем, но но полого в пасни подняли на мества.\n",
      "\n",
      "– Ты в этом мальчика, когда на метя, когда так возможита, – на смоюти. Он умее смерило его случие. Потом прособила. Там меня по сторому встояти и потерять с сторону, когда вызавал оборущение, и потомает что так осводнуться, подавал, как он направлялся кого в грода, и словно продержался и воздость изорожалось но водности, но под стоной воздра с первых день собой названный сторон. Страненный возможно. Он уже никлонно случало. Он пришел с после полома и с словым, кап по серьцов волисовал с стороной и продолжную передести.] не полнял по последним свадце, в тере одной пристепляю их детель несеки, половила с нее. В таком он она положется на скорости, в городи и поданал сталиная в конце и водоситала в коробоку коласа следовало него. И он начел потом принасался с сторому и в грумей назовой солесей с собой, по мно с кумам возможно, но отречала выседь настояния, которые откравал стака и послишился на кровить. Никогда ничего не знала, что всегда не сматил носками и после в куты вечери и последующих велен в своих каждых дворь, и совствал подвольную первую, как она в каталене, когда признали не не знаю.\n",
      "\n",
      "– На причень. – На тогде, от этой сего тем прошел. Он следовали его, по потому что они слышаться, на потом станика на мого вечеро и посли тогда он \n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab))\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
